[2025-11-20 02:08:35 swin_mamba_tiny_patch4_window7_224_cifar100] (main.py 348): INFO Full config saved to output/swin_mamba_tiny_patch4_window7_224_cifar100/default/config.json
[2025-11-20 02:08:35 swin_mamba_tiny_patch4_window7_224_cifar100] (main.py 351): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: cifar100
  DATA_PATH: /root/autodl-fs/vim-shiyan/vim-main_1/data/cifar-100-python
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_mamba_tiny_patch4_window7_224_cifar100
  NUM_CLASSES: 100
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MAMBA:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    D_STATE: 8
    EMBED_DIM: 96
    FUSED_WINDOW_PROCESS: false
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    RMS_NORM: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin_mamba
OUTPUT: output/swin_mamba_tiny_patch4_window7_224_cifar100/default
PRINT_FREQ: 50
SAVE_FREQ: 10
SEED: 0
TAG: default
TEST:
  CROP: false
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 1.0
  EPOCHS: 300
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.5e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: AdamW
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 2.5e-07
  WEIGHT_DECAY: 0.05

[2025-11-20 02:08:35 swin_mamba_tiny_patch4_window7_224_cifar100] (main.py 352): INFO {"cfg": "/root/autodl-fs/vim-shiyan/swin_mamba_tiny_patch4_window7_cifar100.yaml", "opts": null, "batch_size": null, "data_path": "/root/autodl-fs/vim-shiyan/vim-main_1/data/cifar-100-python", "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2025-11-20 02:08:36 swin_mamba_tiny_patch4_window7_224_cifar100] (main.py 93): INFO Creating model:swin_mamba/swin_mamba_tiny_patch4_window7_224_cifar100
[2025-11-20 02:08:36 swin_mamba_tiny_patch4_window7_224_cifar100] (main.py 95): INFO SwinMamba(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      (blocks): ModuleList(
        (0): SwinMambaBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mamba): MambaBlock(
            (mixer): Mamba(
              (in_proj): Linear(in_features=96, out_features=384, bias=False)
              (conv1d): Conv1d(192, 192, kernel_size=(4,), stride=(1,), padding=(3,), groups=192)
              (act): SiLU()
              (x_proj): Linear(in_features=192, out_features=22, bias=False)
              (dt_proj): Linear(in_features=6, out_features=192, bias=True)
              (conv1d_b): Conv1d(192, 192, kernel_size=(4,), stride=(1,), padding=(3,), groups=192)
              (x_proj_b): Linear(in_features=192, out_features=22, bias=False)
              (dt_proj_b): Linear(in_features=6, out_features=192, bias=True)
              (out_proj): Linear(in_features=192, out_features=96, bias=False)
            )
            (norm): RMSNorm()
            (drop_path): Identity()
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinMambaBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mamba): MambaBlock(
            (mixer): Mamba(
              (in_proj): Linear(in_features=96, out_features=384, bias=False)
              (conv1d): Conv1d(192, 192, kernel_size=(4,), stride=(1,), padding=(3,), groups=192)
              (act): SiLU()
              (x_proj): Linear(in_features=192, out_features=22, bias=False)
              (dt_proj): Linear(in_features=6, out_features=192, bias=True)
              (conv1d_b): Conv1d(192, 192, kernel_size=(4,), stride=(1,), padding=(3,), groups=192)
              (x_proj_b): Linear(in_features=192, out_features=22, bias=False)
              (dt_proj_b): Linear(in_features=6, out_features=192, bias=True)
              (out_proj): Linear(in_features=192, out_features=96, bias=False)
            )
            (norm): RMSNorm()
            (drop_path): DropPath()
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      (blocks): ModuleList(
        (0-1): 2 x SwinMambaBlock(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mamba): MambaBlock(
            (mixer): Mamba(
              (in_proj): Linear(in_features=192, out_features=768, bias=False)
              (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
              (act): SiLU()
              (x_proj): Linear(in_features=384, out_features=28, bias=False)
              (dt_proj): Linear(in_features=12, out_features=384, bias=True)
              (conv1d_b): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
              (x_proj_b): Linear(in_features=384, out_features=28, bias=False)
              (dt_proj_b): Linear(in_features=12, out_features=384, bias=True)
              (out_proj): Linear(in_features=384, out_features=192, bias=False)
            )
            (norm): RMSNorm()
            (drop_path): DropPath()
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      (blocks): ModuleList(
        (0-5): 6 x SwinMambaBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mamba): MambaBlock(
            (mixer): Mamba(
              (in_proj): Linear(in_features=384, out_features=1536, bias=False)
              (conv1d): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
              (act): SiLU()
              (x_proj): Linear(in_features=768, out_features=40, bias=False)
              (dt_proj): Linear(in_features=24, out_features=768, bias=True)
              (conv1d_b): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
              (x_proj_b): Linear(in_features=768, out_features=40, bias=False)
              (dt_proj_b): Linear(in_features=24, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=384, bias=False)
            )
            (norm): RMSNorm()
            (drop_path): DropPath()
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      (blocks): ModuleList(
        (0-1): 2 x SwinMambaBlock(
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mamba): MambaBlock(
            (mixer): Mamba(
              (in_proj): Linear(in_features=768, out_features=3072, bias=False)
              (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
              (act): SiLU()
              (x_proj): Linear(in_features=1536, out_features=64, bias=False)
              (dt_proj): Linear(in_features=48, out_features=1536, bias=True)
              (conv1d_b): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
              (x_proj_b): Linear(in_features=1536, out_features=64, bias=False)
              (dt_proj_b): Linear(in_features=48, out_features=1536, bias=True)
              (out_proj): Linear(in_features=1536, out_features=768, bias=False)
            )
            (norm): RMSNorm()
            (drop_path): DropPath()
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=100, bias=True)
)
[2025-11-20 02:08:36 swin_mamba_tiny_patch4_window7_224_cifar100] (main.py 98): INFO number of params: 33498628
[2025-11-20 02:08:36 swin_mamba_tiny_patch4_window7_224_cifar100] (main.py 135): INFO no checkpoint found in output/swin_mamba_tiny_patch4_window7_224_cifar100/default, ignoring auto resume
[2025-11-20 02:08:36 swin_mamba_tiny_patch4_window7_224_cifar100] (main.py 153): INFO Start training
