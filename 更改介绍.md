整理一下这次针对 Swin-Mamba 的修改和优化点，方便你后续继续实验。

  一、MambaBlock：更贴近 Vision Mamba 的实现

  文件：Swin-Transformer-main/models/mamba_modules.py

  - 语义对齐：
      - 注释和结构对齐 Vim 中的 Block + create_block 思路，明确这是用来替换 Swin 的 WindowAttention 的 Mamba 块。
  - BiMamba 配置逻辑：
      - 新增 if_bimamba + bimamba_type="none" 组合逻辑：
          - 当 if_bimamba=True 且未显式指定类型时，会自动设为 "v1"（和 Vim 的 create_block 一致）。
          - 你也可以直接在上层传 bimamba_type="v2"，这时 if_bimamba 不再强制覆盖（下面的 SwinBiMamba 就是这种用法）。
  - fused_add_norm 更安全：
      - 如果 fused_add_norm=True 但 RMSNorm 未成功导入（例如没编译 Triton 扩展），会自动将 fused_add_norm 置为 False，避
        免运行时 assert 报错。
  - 归一化层保持与 Vim 一致：
      - self.norm = (nn.LayerNorm if not rms_norm else RMSNorm)(dim, eps=norm_epsilon, ...)
      - 这样在上层传 rms_norm=True 时，就会走 Vim 一样的 RMSNorm 路径。

  整体上，MambaBlock 现在是一种「Vision Mamba 风格的 Mamba Block」，你可以通过 rms_norm、residual_in_fp32、
  fused_add_norm、bimamba_type、if_divide_out 在上层精细配置。

  ———

  二、SwinMambaBlock / BasicLayer / SwinMamba：支持 Vim 风格超参与 BiMamba

  文件：Swin-Transformer-main/models/swin_mamba.py

  1. SwinMambaBlock

  - 接口扩展：
      - 新增参数：
          - residual_in_fp32=False
          - fused_add_norm=False
          - norm_epsilon=1e-5
          - ssm_cfg=None
          - init_layer_scale=None
      - 这些参数会原样传入 MambaBlock，实现和 Vim 中 create_block 类似的灵活配置。
  - MambaBlock 构造现在是：

  self.mamba = MambaBlock(
      dim=dim,
      d_state=d_state,
      ssm_cfg=ssm_cfg,
      norm_epsilon=norm_epsilon,
      rms_norm=rms_norm,
      if_bimamba=if_bimamba,
      bimamba_type=bimamba_type,
      if_divide_out=if_divide_out,
      residual_in_fp32=residual_in_fp32,
      fused_add_norm=fused_add_norm,
      init_layer_scale=init_layer_scale,
      drop_path=drop_path,
  )

  - 关于移位 mask：
      - 仍然计算并注册 attn_mask，以保持与原 Swin 结构兼容。
      - 注释中已经明确说明：当前 Mamba 不显式使用该 mask，仅为兼容保留，避免误解。

  2. BasicLayer

  - 接口扩展：
      - 新增并下传：
          - residual_in_fp32=False
          - fused_add_norm=False
          - norm_epsilon=1e-5
          - ssm_cfg=None
          - init_layer_scale=None
  - 每个 SwinMambaBlock 都会拿到同样一套 Mamba 相关超参，保证层内配置一致。

  3. SwinMamba 主网络

  - 构造函数扩展：

  def __init__(...,
               use_checkpoint=False, d_state=16, rms_norm=False,
               if_bimamba=False, bimamba_type="v2", if_divide_out=False,
               residual_in_fp32=False, fused_add_norm=False,
               norm_epsilon=1e-5, ssm_cfg=None, init_layer_scale=None, **kwargs):

  - 在构建每一层 BasicLayer 时，会把这些 Vim/Mamba 风格的参数全部透传进去。
  - 原有参数（img_size、depths、num_heads、window_size 等）不变，因此你以前的 YAML / 训练脚本不会被破坏。

  ———

  三、BiMamba（Vim 风格）Swin 模型注册

  文件：Swin-Transformer-main/models/swin_mamba.py 末尾

  1. 原有的三个基础模型保持简单默认配置：

  - swin_mamba_tiny_patch4_window7_224
  - swin_mamba_small_patch4_window7_224
  - swin_mamba_base_patch4_window7_224

  都只是传 d_state 等基础参数，你可以在外部通过 YAML 覆盖 RMS_NORM 等。

  2. 重点：Vim 风格 BiMamba 版本

  @register_model
  def swin_mamba_tiny_bimambav2_patch4_window7_224(pretrained=False, **kwargs):
      # 集成 Vision Mamba 的双向 BiMamba 配置：
      # - 使用 RMSNorm
      # - residual_in_fp32 / fused_add_norm 为 True
      # - bimamba_type="v2" 且 if_divide_out=True
      model = SwinMamba(
          img_size=224, patch_size=4, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
          window_size=7, d_state=16,
          rms_norm=True,
          residual_in_fp32=True,
          fused_add_norm=True,
          if_bimamba=False,            # 直接使用 bimamba_type="v2"
          bimamba_type="v2",
          if_divide_out=True,
          **kwargs)
      return model

  - 这基本就是你要的「在 Swin 架构里嵌入 Vision Mamba 风格的双向 Mamba」：
      - 使用 RMSNorm。
      - 使用 FP32 残差 + fused add-norm（前提是 mamba_ssm 的 Triton 扩展编译成功，否则自动退回非 fused）。
      - 使用 bimamba_type="v2" + if_divide_out=True，和 Vim 中 *_bimambav2_*_div2 模型命名对应。

  ———

  四、如何在你的 Swin 框架中使用新版 Swin-Mamba

  1. 纯 Swin-Mamba（单向）版本

  - 模型名（和之前一样）：

  python -m torch.distributed.launch ... \
    --model swin_mamba_tiny_patch4_window7_224 \
    --cfg configs/xxx.yaml \
    ...

  - 通过 YAML 控制：
      - MODEL.TYPE: swin_mamba
      - MODEL.SWIN_MAMBA.D_STATE, RMS_NORM 等。

  2. BiMamba Vim 风格版本（推荐用于对齐 Vision Mamba 实验）

  - 使用新的注册模型名：

  python -m torch.distributed.launch ... \
    --model swin_mamba_tiny_bimambav2_patch4_window7_224 \
    --cfg swin_mamba_tiny_patch4_window7_cifar100.yaml \
    ...

  - 若你想在 CIFAR-100 上用 BiMamba，只需要在 YAML 中改 MODEL.NAME 或 MODEL.TYPE / MODEL.MODEL_NAME 对应字段为这个新模型
    名（具体字段取决于你当前 config.py 的用法）。

  ———

  五、后续你可以进一步做的事

  - 如果你希望更「极致对齐」 Vim：
      - 可以在 ssm_cfg 里加入 Vim 中使用的 d_conv, expand, dt_rank 等配置，并在 YAML 暴露这些字段。
      - 可以对 init_layer_scale 设为一个小值（如 1e-5），与 Vim 的初始化策略一致。
  - 你可以先在一个小数据集（如 CIFAR-100，用你现有的 swin_mamba_tiny_patch4_window7_cifar100.yaml）上分别跑：
      - swin_mamba_tiny_patch4_window7_224
      - swin_mamba_tiny_bimambav2_patch4_window7_224
        对比收敛速度和最终精度，验证 BiMamba + RMSNorm + fused_add_norm 组合的效果。

  如果你愿意，我可以帮你：

  - 根据 config.py 的实际字段，写出一条完整可运行的命令行（ImageNet 或 CIFAR-100）；
  - 或者进一步把 Vim 的 ssm_cfg 关键字段抽出来，接到 Swin-Mamba 的 YAML 配置里，让你可以从配置文件层面全面控制 Mamba 的内
    部结构。